{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar_10_with_resnet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "wVk47Jxt9tVN",
        "outputId": "518021ef-d7fc-456f-d2cb-2be583b0ad48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LgHhw7Vr95jT",
        "outputId": "68d3c045-b914-4602-b284-0d42d9a328a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tK0nKp-U-Bfj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='tanh',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tlccO5V_-ZuK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation='relu')\n",
        "            y = Dropout(.25)(y)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation='tanh',\n",
        "                                 batch_normalization=True)\n",
        "                x = Dropout(.35)(x)\n",
        "                \n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QOzQMg3Y-gPg",
        "outputId": "8933f3e6-3158-41cf-ee3c-abc5acfabf57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "model = resnet_v1(input_shape=(32,32,3), depth=44)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hbxVwdIa-kY3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qd5UlDtB-xMh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_XudIYM-mUXd",
        "colab_type": "code",
        "outputId": "5b6a06da-5343-40c4-a13f-ea27e13dea20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "!ls\n",
        "os.chdir('/root/.keras/datasets/')\n",
        "!ls\n",
        "!rm -rf cifar-10-batches-py\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-19877fb69460>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/root/.keras/datasets/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm -rf cifar-10-batches-py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/.keras/datasets/'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zM9fsA9DCKpE",
        "outputId": "0fa6165f-7387-4bb7-e513-8b5b563cd8db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')/255.\n",
        "x_test = x_test.astype('float32')/255.\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 30s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1NPmk497CeJi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "data_augmentation = True\n",
        "#num_predictions = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "a9vPh-cvCQEW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w_w16XH1CYnt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.adam(lr=0.001, decay=1e-6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oF0e1dSECi3k",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "k6htbGNOCmEC",
        "outputId": "e57f5482-1b66-405f-e4d0-6a343d16a4ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1363
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "early_stopping_monitor = EarlyStopping(patience=5, verbose=1)\n",
        "\n",
        "# checkpoint\n",
        "filepath=\"weights.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              callbacks=callbacks_list,\n",
        "              shuffle=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "50000/50000 [==============================] - 196s 4ms/step - loss: 1.8736 - acc: 0.4349 - val_loss: 1.5649 - val_acc: 0.5372\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.53720, saving model to weights.best.hdf5\n",
            "Epoch 2/100\n",
            "50000/50000 [==============================] - 184s 4ms/step - loss: 1.4192 - acc: 0.5934 - val_loss: 1.2489 - val_acc: 0.6512\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.53720 to 0.65120, saving model to weights.best.hdf5\n",
            "Epoch 3/100\n",
            "50000/50000 [==============================] - 184s 4ms/step - loss: 1.2275 - acc: 0.6586 - val_loss: 1.2411 - val_acc: 0.6530\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.65120 to 0.65300, saving model to weights.best.hdf5\n",
            "Epoch 4/100\n",
            "50000/50000 [==============================] - 184s 4ms/step - loss: 1.1228 - acc: 0.6956 - val_loss: 1.0331 - val_acc: 0.7254\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.65300 to 0.72540, saving model to weights.best.hdf5\n",
            "Epoch 5/100\n",
            "50000/50000 [==============================] - 184s 4ms/step - loss: 1.0537 - acc: 0.7200 - val_loss: 0.9956 - val_acc: 0.7399\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.72540 to 0.73990, saving model to weights.best.hdf5\n",
            "Epoch 6/100\n",
            "50000/50000 [==============================] - 184s 4ms/step - loss: 1.0053 - acc: 0.7360 - val_loss: 0.9433 - val_acc: 0.7549\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.73990 to 0.75490, saving model to weights.best.hdf5\n",
            "Epoch 7/100\n",
            "50000/50000 [==============================] - 185s 4ms/step - loss: 0.9733 - acc: 0.7462 - val_loss: 0.8967 - val_acc: 0.7722\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.75490 to 0.77220, saving model to weights.best.hdf5\n",
            "Epoch 8/100\n",
            "50000/50000 [==============================] - 183s 4ms/step - loss: 0.9420 - acc: 0.7576 - val_loss: 0.8607 - val_acc: 0.7865\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.77220 to 0.78650, saving model to weights.best.hdf5\n",
            "Epoch 9/100\n",
            "50000/50000 [==============================] - 183s 4ms/step - loss: 0.9176 - acc: 0.7680 - val_loss: 0.8710 - val_acc: 0.7877\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.78650 to 0.78770, saving model to weights.best.hdf5\n",
            "Epoch 10/100\n",
            "50000/50000 [==============================] - 185s 4ms/step - loss: 0.8962 - acc: 0.7772 - val_loss: 0.8706 - val_acc: 0.7807\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.78770\n",
            "Epoch 11/100\n",
            "50000/50000 [==============================] - 183s 4ms/step - loss: 0.8780 - acc: 0.7822 - val_loss: 1.0887 - val_acc: 0.7247\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.78770\n",
            "Epoch 12/100\n",
            "50000/50000 [==============================] - 183s 4ms/step - loss: 0.8642 - acc: 0.7860 - val_loss: 0.8132 - val_acc: 0.8081\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.78770 to 0.80810, saving model to weights.best.hdf5\n",
            "Epoch 13/100\n",
            "50000/50000 [==============================] - 182s 4ms/step - loss: 0.8497 - acc: 0.7919 - val_loss: 0.8145 - val_acc: 0.8076\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.80810\n",
            "Epoch 14/100\n",
            "50000/50000 [==============================] - 191s 4ms/step - loss: 0.8401 - acc: 0.7955 - val_loss: 0.9359 - val_acc: 0.7705\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.80810\n",
            "Epoch 15/100\n",
            "50000/50000 [==============================] - 186s 4ms/step - loss: 0.8237 - acc: 0.8028 - val_loss: 0.9374 - val_acc: 0.7679\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.80810\n",
            "Epoch 16/100\n",
            "50000/50000 [==============================] - 184s 4ms/step - loss: 0.8123 - acc: 0.8041 - val_loss: 0.8392 - val_acc: 0.7989\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.80810\n",
            "Epoch 17/100\n",
            "50000/50000 [==============================] - 186s 4ms/step - loss: 0.8042 - acc: 0.8075 - val_loss: 0.8629 - val_acc: 0.7907\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.80810\n",
            "Epoch 18/100\n",
            "50000/50000 [==============================] - 184s 4ms/step - loss: 0.7932 - acc: 0.8135 - val_loss: 0.8730 - val_acc: 0.7831\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.80810\n",
            "Epoch 19/100\n",
            "35184/50000 [====================>.........] - ETA: 54s - loss: 0.7791 - acc: 0.8166Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fYQ_BnPzDv6A",
        "outputId": "fa280ced-a0d3-46d0-d02b-1c71fee294e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 245us/step\n",
            "Test loss: 0.7068420631408692\n",
            "Test accuracy: 0.8487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QOsrUO9KFMpw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}